{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 入门\n",
    "\n",
    "## 张量\n",
    "\n",
    "`Tensor` (张量）类似于 `NumPy` 的 `ndarray` ，但还可以在GPU上使用来加速计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0194e-38, 1.0469e-38, 1.0010e-38],\n",
      "        [8.9081e-39, 8.9082e-39, 5.9694e-39],\n",
      "        [8.9082e-39, 1.0194e-38, 9.1837e-39],\n",
      "        [4.6837e-39, 9.9184e-39, 9.0000e-39],\n",
      "        [1.0561e-38, 1.0653e-38, 4.1327e-39]])\n"
     ]
    }
   ],
   "source": [
    "# 常见一个没有初始化的矩阵:\n",
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6333, 0.1238, 0.3310],\n",
      "        [0.6725, 0.4348, 0.9720],\n",
      "        [0.9977, 0.1665, 0.5009],\n",
      "        [0.1138, 0.9547, 0.3196],\n",
      "        [0.9054, 0.9591, 0.4052]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个随机初始化矩阵：\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# 构造一个填满 0 且数据类型为 long 的矩阵:\n",
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "# 直接从数据构造张量：\n",
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 0.0709,  0.0649,  1.1349],\n",
      "        [-1.5819,  0.7733,  1.0178],\n",
      "        [ 0.4519, -0.5855, -0.5695],\n",
      "        [-0.1797,  0.0508,  1.0114],\n",
      "        [ 0.6204,  0.8975,  0.7071]])\n"
     ]
    }
   ],
   "source": [
    "# 或者根据已有的tensor建立新的tensor。除非用户提供新的值，否则这些方法将重用输入张量的属性，例如dtype等：\n",
    "x = x.new_ones(5, 3, dtype=torch.double)  # new_* methods take in sizes\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype=torch.float)  # 重载 dtype! but size is same\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# 获取张量的形状:\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# torch.Size 本质上还是 tuple ，所以支持tuple的一切操作。\n",
    "print(x.size()[0])\n",
    "print(x.size()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运算\n",
    "\n",
    "一种运算有多种语法。在下面的示例中，我们将研究加法运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2950, 1.0442, 1.7022],\n",
      "        [1.5774, 1.5522, 1.5736],\n",
      "        [1.3308, 1.2064, 1.8987],\n",
      "        [1.3453, 1.4068, 1.8807],\n",
      "        [1.6129, 1.5709, 1.2858]])\n",
      "tensor([[1.2950, 1.0442, 1.7022],\n",
      "        [1.5774, 1.5522, 1.5736],\n",
      "        [1.3308, 1.2064, 1.8987],\n",
      "        [1.3453, 1.4068, 1.8807],\n",
      "        [1.6129, 1.5709, 1.2858]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, 3)\n",
    "y = torch.rand(5, 3)  # `torch.rand()` 0-1随机浮点数； randn() 是正态分布随机数\n",
    "\n",
    "# 加法：形式一\n",
    "print(x + y)\n",
    "\n",
    "# 加法：形式二\n",
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2950, 1.0442, 1.7022],\n",
      "        [1.5774, 1.5522, 1.5736],\n",
      "        [1.3308, 1.2064, 1.8987],\n",
      "        [1.3453, 1.4068, 1.8807],\n",
      "        [1.6129, 1.5709, 1.2858]])\n"
     ]
    }
   ],
   "source": [
    "# 加法：给定一个输出张量作为参数\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2950, 1.0442, 1.7022],\n",
      "        [1.5774, 1.5522, 1.5736],\n",
      "        [1.3308, 1.2064, 1.8987],\n",
      "        [1.3453, 1.4068, 1.8807],\n",
      "        [1.6129, 1.5709, 1.2858]])\n"
     ]
    }
   ],
   "source": [
    "# 加法：原位/原地操作(in-place）\n",
    "# adds x to y\n",
    "y.add_(x)\n",
    "print(y)\n",
    "# 注意：任何一个in-place改变张量的操作后面都固定一个 _ 。例如 x.copy_(y) 、 x.t_() 将更改x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0442, 1.5522, 1.2064, 1.4068, 1.5709])\n"
     ]
    }
   ],
   "source": [
    "# 也可以使用像标准的NumPy一样的各种索引操作：\n",
    "print(y[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "# 改变形状：如果想改变形状，可以使用 torch.view\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8) # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4001])\n",
      "0.4001099169254303\n"
     ]
    }
   ],
   "source": [
    "# 如果是仅包含一个元素的tensor，可以使用 .item() 来得到对应的python数值\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 桥接 NumPy\n",
    "\n",
    "将一个Torch张量转换为一个NumPy数组是轻而易举的事情，反之亦然。\n",
    "\n",
    "Torch张量和NumPy数组将共享它们的底层内存位置，因此当一个改变时,另外也会改变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# 将torch的Tensor转化为NumPy数组\n",
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# 看NumPy数组是如何改变里面的值的：\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 将NumPy数组转化为Torch张量\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU上的所有张量(CharTensor除外)都支持与Numpy的相互转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA上的张量\n",
    "\n",
    "张量可以使用 `.to` 方法移动到任何设备(device）上：\n",
    "\n",
    "```python\n",
    "# 当GPU可用时,我们可以运行以下代码\n",
    "# 我们将使用`torch.device`来将tensor移入和移出GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device) # 直接在GPU上创建tensor\n",
    "    x = x.to(device) # 或者使用`.to(\"cuda\")`方法\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double)) # `.to`也能在移动时改变dtype\n",
    "```\n",
    "\n",
    "输出\n",
    "\n",
    "```python\n",
    "tensor([1.0445], device='cuda:0')\n",
    "tensor([1.0445], dtype=torch.float64)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd：自动求导\n",
    "\n",
    "PyTorch中，所有神经网络的核心是 `autograd` 包。\n",
    "\n",
    "先简单介绍一下这个包，然后训练我们的第一个的神经网络。\n",
    "\n",
    "`autograd` 包为张量上的所有操作提供了自动求导机制。它是一个在运行时定义(define-byrun）的框架，这意味着反向传播是根据代码如何运行来决定的，并且每次迭代可以是不同的.\n",
    "\n",
    "`torch.Tensor` 是这个包的核心类。如果设置它的属性 `.requires_grad` 为 True ，那么它将会追踪对于该张量的所有操作。\n",
    "\n",
    "当完成计算后可以通过调用 `.backward()` ，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到 `.grad` 属性.\n",
    "\n",
    "\n",
    "\n",
    "要阻止一个张量被跟踪历史，可以调用 `.detach()` 方法将其与计算历史分离，并阻止它未来的计算记录被跟踪。\n",
    " \n",
    "\n",
    "为了防止跟踪历史记录(和使用内存），可以将代码块包装在 `with torch.no_grad():` 中。在评估模型时特别有用，因为模型可能具有 `requires_grad = True` 的可训练的参数，但是我们不需要在此过程中对他们进行梯度计算。\n",
    "\n",
    "\n",
    "还有一个类对于autograd的实现非常重要： `Function` 。\n",
    "\n",
    "Tensor 和 Function 互相连接生成了一个无圈图(acyclic graph)，它编码了完整的计算历史。\n",
    "\n",
    "每个张量都有一个 `.grad_fn` 属性，该属性引用了创建 Tensor 自身的 `Function` (除非这个张量是用户手动创建的，即这个张量的 `grad_fn` 是 None )。\n",
    "\n",
    "如果需要计算导数，可以在 Tensor 上调用 `.backward()` 。如果 Tensor 是一个标量(即它包含一个元素的数据），则不需要为 `backward()` 指定任何参数，但是如果它有更多的元素，则需要指定一个 `gradient` 参数，该参数是形状匹配的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 创建一个张量并设置 requires_grad=True 用来追踪其计算历史\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 对这个张量做一次运算：\n",
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x0000025114B7EC10>\n"
     ]
    }
   ],
   "source": [
    "# y 是计算的结果，所以它有 grad_fn 属性。\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 对y进行更多操作\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x0000025114B7E9D0>\n"
     ]
    }
   ],
   "source": [
    "# .requires_grad_(...) 原地改变了现有张量的 requires_grad 标志。如果没有指定的话，默认输入的这个标志是 False 。\n",
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "# 现在开始进行反向传播，因为 out 是一个标量，因此 out.backward() 和out.backward(torch.tensor(1.)) 等价。\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "out.backward()  \n",
    "print(x.grad)  # out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们来调用 out 张量 “o” 。\n",
    "\n",
    "就可以得到：\n",
    "\n",
    "$ o = \\frac{1}{4}\\sum_i z_i $\n",
    "\n",
    "$ z_i = 3(x_i+2)^2 $\n",
    "\n",
    "$ z_i\\bigr\\rvert{x_{i=1}} = 27 $ \n",
    "\n",
    "因此, \n",
    "\n",
    "$ \\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2) $\n",
    "\n",
    "因而,\n",
    "\n",
    "$ \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert {x_{i=1}} = \\frac{9}{2} = 4.5 $。\n",
    "\n",
    "数学上，若有向量值函数 $\\vec{y}=f(\\vec{x})$ ，那么 $\\vec{y}$ 相对于 $\\vec{x}$ 的梯度是一个雅可比矩阵：\n",
    "\n",
    "$ J=\\left(\\begin{array}{ccc} \\frac{\\partial y{1}}{\\partial x{1}} & \\cdots & \\frac{\\partial y{m}}{\\partial x{1}}\\\\ \n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial y{1}}{\\partial x{n}} & \\cdots & \\frac{\\partial y{m}}\n",
    "{\\partial x{n}} \\end{array}\\right) $\n",
    "\n",
    "通常来说， `torch.autograd` 是计算雅可比向量积的一个“引擎”。\n",
    "\n",
    "也就是说，给定任意向量：$ v=\\left(\\begin{array}{cccc} v{1} & v{2} & \\cdots & v{m}\\end{array}\\right)^{T} $\n",
    "\n",
    "计算乘积   $v^{T}\\cdot J$\n",
    "\n",
    "如果 v 恰好是一个标量函数 $l=g\\left(\\vec{y}\\right)$ 的导数，即 $v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y {1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$，\n",
    "\n",
    "那么根据链式法则，雅可比向量积应该是 $l$ 对 $\\vec{x} $ 的导数：\n",
    "\n",
    "$J^{T}\\cdot v=\\left(\\begin{array}{ccc} \\frac{\\partial y{1}}{\\partial x{1}} &\n",
    "\\cdots & \\frac{\\partial y{m}}{\\partial x{1}}\\\\ \\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial y{1}}{\\partial x{n}} & \\cdots & \\frac{\\partial y{m}}\n",
    "{\\partial x{n}} \\end{array}\\right)\\left(\\begin{array}{c} \\frac{\\partial l}\n",
    "{\\partial y{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial y{m}}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial x{1}}\\\\\n",
    "\\vdots\\\\ \\frac{\\partial l}{\\partial x{n}} \\end{array}\\right)$\n",
    "\n",
    "(注意：行向量的 $v^{T}\\cdot J$ 也可以被视作列向量的 $J^{T}\\cdot v$ )\n",
    "\n",
    "雅可比向量积的这一特性使得将外部梯度输入到具有非标量输出的模型中变得非常方便。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1664.5364,  352.0489, -878.6403], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这种情况下， `y` 不再是标量。 `torch.autograd` 不能直接计算完整的雅可比矩阵，但是如果我们只想要雅可比向量积，只需将这个向量作为参数传给 `backward` ："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1664.5364,  352.0489, -878.6403], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# 也可以通过将代码块包装在 with torch.no_grad(): 中，来阻止autograd跟踪设置了.requires_grad=True 的张量的历史记录。\n",
    "\n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autograd 和 Function 的文档见： https://pytorch.org/docs/autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络\n",
    "\n",
    "一个神经网络的典型训练过程如下：\n",
    "\n",
    "- 定义包含一些可学习参数(或者叫权重）的神经网络\n",
    "\n",
    "- 在输入数据集上迭代\n",
    "\n",
    "- 通过网络处理输入\n",
    "\n",
    "- 计算损失(输出和正确答案的距离）\n",
    "\n",
    "- 将梯度反向传播给网络的参数\n",
    "\n",
    "- 更新网络的权重，一般使用一个简单的规则： `weight = weight - learning_rate *gradient`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 输入图像channel: 1; 输出channel: 6; 5x5卷积核\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "480px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
